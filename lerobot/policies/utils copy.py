import zmq
import torch
import cv2
import numpy as np
import json
import time
import argparse
import logging
import os
from typing import Dict, Any, List, Tuple
from types import SimpleNamespace

# LeRobot Policies and Utilities
from lerobot.configs.policies import PreTrainedConfig
from lerobot.policies.factory import make_policy, make_pre_post_processors
from lerobot.processor import PolicyProcessorPipeline
from lerobot.utils.utils import get_safe_torch_device
from safetensors import safe_open


# --- ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ---
def decode_image(img_bytes: bytes) -> np.ndarray:
    """ë°”ì´íŠ¸ ë°ì´í„°ë¥¼ OpenCV ì´ë¯¸ì§€ (BGR)ë¡œ ë³µì›í•©ë‹ˆë‹¤."""
    nparr = np.frombuffer(img_bytes, np.uint8)
    img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
    if img is None:
        raise ValueError("Failed to decode image bytes.")
    return img

def load_ds_meta_from_policy_path(policy_path: str) -> Any:
    """
    ì •ì±… ê²½ë¡œì—ì„œ config.jsonê³¼ normalizer safetensorsë¥¼ ë¶„ì„í•˜ì—¬
    make_policyì— í•„ìš”í•œ ds_meta ê°ì²´(SimpleNamespace)ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.
    """
    
    # 1. config.json ë¡œë“œ
    config_path = os.path.join(policy_path, "config.json")
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"config.json not found at {config_path}")
        
    with open(config_path, 'r') as f:
        policy_config = json.load(f)

    # 2. normalizer safetensorsì—ì„œ stats ì¶”ì¶œ
    normalizer_path = os.path.join(policy_path, "policy_preprocessor_step_5_normalizer_processor.safetensors")
    stats_dict = {}
    if os.path.exists(normalizer_path):
        try:
            with safe_open(normalizer_path, framework="pt") as f:
                for key in f.keys():
                    # Key format: obs_key/mean, obs_key/std
                    parts = key.split('/')
                    if len(parts) == 2:
                        name, stat_type = parts
                        tensor = f.get_tensor(key)
                        value = tensor.cpu().numpy().tolist()
                        if name not in stats_dict: stats_dict[name] = {}
                        stats_dict[name][stat_type] = value
            print("âœ… Loaded normalization stats from safetensors.")
        except Exception as e:
             logging.warning(f"Failed to load stats from safetensors: {e}. Normalization may fail.")
    else:
        logging.warning(f"âš ï¸ Normalizer safetensors not found at {normalizer_path}. Stats might be incomplete.")

    # 3. ds_meta ê°ì²´ êµ¬ì„± ë° [ê°•ì œ ë³´ì •]
    if "input_features" not in policy_config or "output_features" not in policy_config:
        raise KeyError("config.json does not contain 'input_features' or 'output_features'.")

    # Raw Features ë³‘í•©
    raw_features = {}
    raw_features.update(policy_config["input_features"])
    raw_features.update(policy_config["output_features"])

    # ðŸŒŸ [í•µì‹¬ ìˆ˜ì •] ì´ë¦„(names) ë° íƒ€ìž…(dtype) ê°•ì œ ì£¼ìž…
    converted_features = {}
    for key, feat_info in raw_features.items():
        new_info = feat_info.copy()
        feat_type = feat_info.get("type")
        
        # (1) Dtype ë³´ì •
        if feat_type == "VISUAL":
            new_info["dtype"] = "video" 
        else:
            new_info["dtype"] = "float32"

        # (2) Names ê°•ì œ ìƒì„± (STATE, ACTIONì´ê³  namesê°€ ì—†ì„ ë•Œ)
        if feat_type in ["STATE", "ACTION"]:
            if "names" not in new_info:
                # shape ì •ë³´ í™•ì¸ (ì˜ˆ: [10])
                shape = new_info.get("shape", [])
                if shape:
                    dim = shape[0]
                    # ê°€ì§œ ì´ë¦„ ìƒì„± (ì˜ˆ: action_0, action_1...)
                    # ì£¼ì˜: ë¡œë´‡ PCì—ì„œ ì´ ìˆœì„œëŒ€ë¡œ ëª¨í„°ì— ë§¤í•‘í•´ì•¼ í•¨
                    new_info["names"] = [f"{key}_{i}" for i in range(dim)]
                    print(f"âš ï¸ Generated dummy names for {key}: {new_info['names']}")
                else:
                    logging.warning(f"Feature {key} has no shape information!")

        converted_features[key] = new_info

    # SimpleNamespaceë¡œ ê°ì²´í™”
    ds_meta = SimpleNamespace(
        features=converted_features,
        stats=stats_dict,
        fps=30
    )
    
    return ds_meta

# --- ë©”ì¸ ì»¨íŠ¸ë¡¤ëŸ¬ í´ëž˜ìŠ¤ ---
class InferenceServerController:
    def __init__(self, args):
        self.args = args
        self.device = get_safe_torch_device('cuda' if args.use_gpu else 'cpu')
        
        # --- 1. ëª¨ë¸ ë° í”„ë¡œì„¸ì„œ ë¡œë“œ ---
        self.policy, self.preprocessor, self.postprocessor = self._load_policy_components(args.policy_path)
        
        # --- 2. ZMQ ì„¤ì • (REP ëª¨ë“œ) ---
        self.context = zmq.Context()
        self.socket = self.context.socket(zmq.REP)
        self.bind_address = f"tcp://*:{args.port}"
        print(f"ðŸ“¡ Inference Server binding to {self.bind_address} on {self.device}...")
        self.socket.bind(self.bind_address)
        
        print("âœ… Server Ready. Waiting for Robot Connection...")

    def _load_policy_components(self, policy_path: str) -> Tuple[Any, PolicyProcessorPipeline, PolicyProcessorPipeline]:
        print(f"ðŸ”„ Loading Policy components from {policy_path}...")
        
        # 1. ds_meta ê°ì²´ êµ¬ì„± (Names ìžë™ ìƒì„± í¬í•¨)
        ds_meta = load_ds_meta_from_policy_path(policy_path)
        dataset_stats = ds_meta.stats
        
        # 2. ì •ì±… ì„¤ì • ë¡œë“œ
        conf = PreTrainedConfig.from_pretrained(policy_path)
        conf.pretrained_path = policy_path
        
        # 3. ì •ì±… ìƒì„± (ì´ì œ names ì—ëŸ¬ê°€ ë‚˜ì§€ ì•Šì•„ì•¼ í•¨)
        policy = make_policy(conf, ds_meta=ds_meta) 

        # 4. ì „ì²˜ë¦¬/í›„ì²˜ë¦¬ ìƒì„±
        preprocessor, postprocessor = make_pre_post_processors(
            policy_cfg=conf,
            pretrained_path=conf.pretrained_path,
            dataset_stats=dataset_stats,
            preprocessor_overrides={
                "device_processor": {"device": self.device},
                "rename_observations_processor": {"rename_map": {}}, 
            },
        )
        
        # 5. GPU ì´ë™ ë° eval ëª¨ë“œ
        policy.to(self.device).eval()
        preprocessor.to(self.device)
        postprocessor.to(self.device)
        
        print("âœ… Policy components loaded successfully.")
        return policy, preprocessor, postprocessor

    def _process_observation(self, header: Dict[str, Any], parts: List[bytes]) -> Dict[str, Any]:
        """ZMQ ë©€í‹°íŒŒíŠ¸ ë°ì´í„°ë¥¼ LeRobot Observation Dictë¡œ ë³€í™˜"""
        obs_dict = {}

        # 1. State (ëª¨í„° ê°’)
        state_vec = torch.tensor(header["state"], dtype=torch.float32).to(self.device)
        obs_dict["observation.state"] = state_vec.unsqueeze(0) 

        # 2. Images (ì¹´ë©”ë¼)
        img_idx = 1
        for cam_key in header["image_keys"]:
            if img_idx >= len(parts):
                logging.warning(f"Missing image data for key: {cam_key}")
                continue
                
            img_bytes = parts[img_idx]
            img_bgr = decode_image(img_bytes)
            img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)
            
            img_tensor = torch.from_numpy(img_rgb).permute(2, 0, 1).float() / 255.0
            
            full_key = f"observation.images.{cam_key}"
            obs_dict[full_key] = img_tensor.unsqueeze(0).to(self.device)
            
            img_idx += 1
            
        return obs_dict

    def run(self):
        try:
            while True:
                # 1. ìˆ˜ì‹ 
                try:
                    parts = self.socket.recv_multipart(flags=zmq.NOBLOCK)
                except zmq.Again:
                    time.sleep(0.001)
                    continue

                if not parts: continue

                # 2. ë°ì´í„° ë³µì›
                try:
                    header = json.loads(parts[0].decode('utf-8'))
                    obs_dict = self._process_observation(header, parts)
                except (json.JSONDecodeError, ValueError) as e:
                    logging.error(f"Data processing error: {e}")
                    self.socket.send_json({"error": f"Data decode failed: {e}"})
                    continue

                # 3. ì¶”ë¡ 
                start = time.perf_counter()
                with torch.no_grad():
                    processed_obs = self.preprocessor(obs_dict)
                    action_output = self.policy.select_action(processed_obs) 
                    action_vector = self.postprocessor(action_output)

                # 4. ì „ì†¡ (Dict ë³€í™˜)
                action_dict = {}
                for k, v in action_vector.items():
                    # í…ì„œ ì²˜ë¦¬: (1, Dim) -> List
                    if v.dim() > 0:
                        action_dict[k] = v.squeeze(0).cpu().numpy().tolist()
                    else:
                        action_dict[k] = v.item()

                self.socket.send_json(action_dict)
                
                inference_time_ms = (time.perf_counter() - start) * 1000
                print(f"Inference done in {inference_time_ms:.1f}ms | Sent action.", end='\r')

        except KeyboardInterrupt:
            print("\nServer Stopped.")
        except Exception as e:
            logging.error(f"Critical error: {e}")
        finally:
            self.socket.close()
            self.context.term()

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--policy.path", dest="policy_path", required=True, help="Path to model checkpoint")
    parser.add_argument("--port", default=5555, type=int)
    parser.add_argument("--use_gpu", action="store_true")
    args = parser.parse_args()
    
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    
    InferenceServerController(args).run()